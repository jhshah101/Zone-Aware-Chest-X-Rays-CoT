# -*- coding: utf-8 -*-
"""Zone_aware_Chest-X_rays_CoT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QcTM8KYTXfPgJrbOT_urAEbtO_4Q-C9l
"""

# ============================================================
# 5-LOBE ZONE PIPELINE (YOLO -> ResNet18 features -> Zone-CoT -> GradCAM)
# ============================================================
# Designed as a refactor of your attached code (YOLO + multimodal) into:
#   - YOLO 5-lobe detection
#   - ResNet18 feature extractor
#   - Zone-wise sequential reasoning ("CoT-like")
#   - Grad-CAM visualization
# ============================================================

# ---------- install (Colab) ----------
# !pip install -q ultralytics transformers sentencepiece torchmetrics opencv-python

"""# ============================================================
# 5-LOBE ZONE PIPELINE (YOLO -> ResNet18 features -> Zone-CoT -> GradCAM)
# ============================================================
# Designed as a refactor of your attached code (YOLO + multimodal) into:
#   - YOLO 5-lobe detection
#   - ResNet18 feature extractor
#   - Zone-wise sequential reasoning ("CoT-like")
#   - Grad-CAM visualization
# ============================================================

# ---------- install (Colab) ----------
# !pip install -q ultralytics transformers sentencepiece torchmetrics opencv-python
"""

import os, glob, random
import cv2
import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import List, Tuple, Dict, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

from ultralytics import YOLO

import torchvision
from torchvision import transforms
from torchvision.models import resnet18

from transformers import T5Tokenizer, T5ForConditionalGeneration

# --------------------------
# Repro + device
# --------------------------
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# --------------------------
# USER CONFIG (edit paths)
# --------------------------
CSV_PATH = "/content/drive/MyDrive/new viral.csv"
DATASET_DIR = "/content/drive/MyDrive/Dataset lable/Dataset lable"
YOLO_MODEL_PATH = "/content/drive/MyDrive/best.pt"  # trained lobe detector

BATCH_SIZE = 8
EPOCHS = 20
LR = 1e-4
CONF_TH = 0.25
IMG_SIZE = 224

# Your dataset mapping (same idea as your attached code)
folder_map = {
    "viral": "Pneumoina-Viral",
    "bacterial": "Pneumonia-Bacterial",
    "covid-19": "COVID-19",
    "normal": "Normal"
}

# --------------------------
# 0) Build records from CSV (tolerant filename matching)
# --------------------------
df = pd.read_csv(CSV_PATH, encoding="latin1")
records = []
missing = 0

for _, row in df.iterrows():
    case_id_raw = str(row["Case ID"]).strip()
    label_raw = str(row["Type"]).strip()
    folder = folder_map.get(label_raw.lower(), "Normal")
    folder_path = os.path.join(DATASET_DIR, folder)

    if not os.path.isdir(folder_path):
        candidates = [d for d in os.listdir(DATASET_DIR) if d.lower().startswith(folder.lower()[:4])]
        if candidates:
            folder_path = os.path.join(DATASET_DIR, candidates[0])

    pattern = os.path.join(folder_path, f"{case_id_raw}*")
    matches = glob.glob(pattern)
    if len(matches) == 0:
        missing += 1
        continue

    img_path = matches[0]
    records.append({
        "ImagePath": img_path,
        "Type": label_raw
    })

print(f"Matched records: {len(records)}  (missing {missing})")

# --------------------------
# 1) Encode labels + split
# --------------------------
le = LabelEncoder()
le.fit([r["Type"] for r in records])

for r in records:
    r["LabelIdx"] = int(le.transform([r["Type"]])[0])

train_recs, val_recs = train_test_split(
    records,
    test_size=0.2,
    random_state=SEED,
    stratify=[r["LabelIdx"] for r in records]
)

num_classes = len(le.classes_)
print("Classes:", list(le.classes_))
print("Train:", len(train_recs), "Val:", len(val_recs))

# --------------------------
# 2) Load YOLO (5 lobes)
# --------------------------
yolo = YOLO(YOLO_MODEL_PATH)

# IMPORTANT:
# Your YOLO model must output 5 classes corresponding to lobes, e.g.:
# 0=RUL, 1=RML, 2=RLL, 3=LUL, 4=LLL (or any order you used).
# We'll keep them sorted by class id so the "chain" is consistent.

# --------------------------
# 3) ResNet18 feature extractor (per lobe crop)
# --------------------------
class ResNet18Backbone(nn.Module):
    """ResNet18 -> 512-d feature vector."""
    def __init__(self, pretrained=True):
        super().__init__()
        m = resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT if pretrained else None)
        self.stem = nn.Sequential(
            m.conv1, m.bn1, m.relu, m.maxpool,
            m.layer1, m.layer2, m.layer3, m.layer4
        )
        self.pool = nn.AdaptiveAvgPool2d(1)

    def forward(self, x):
        # x: [B,3,224,224]
        feat_map = self.stem(x)               # [B,512,H,W]
        vec = self.pool(feat_map).flatten(1)  # [B,512]
        return vec, feat_map                 # return both for Grad-CAM

# Normalization for ImageNet pretrained resnet
img_tf = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# --------------------------
# 4) YOLO detect + crop 5 lobes
# --------------------------
@dataclass
class LobeCrop:
    cls_id: int
    box: Tuple[int, int, int, int]  # x1,y1,x2,y2 in original image coords
    crop_rgb: np.ndarray

def detect_5_lobes(img_rgb: np.ndarray, yolo_model, conf_th=0.25) -> List[LobeCrop]:
    """
    Returns up to 5 lobe crops with class ids.
    If a lobe is missing, it will be handled later (zeros).
    """
    results = yolo_model(img_rgb, conf=conf_th, verbose=False)
    res = results[0]

    lobes: List[LobeCrop] = []
    if hasattr(res, "boxes") and res.boxes is not None and len(res.boxes) > 0:
        xyxy = res.boxes.xyxy.detach().cpu().numpy()
        cls = res.boxes.cls.detach().cpu().numpy().astype(int)

        for box, c in zip(xyxy, cls):
            x1, y1, x2, y2 = map(int, box[:4])
            x1 = max(0, x1); y1 = max(0, y1)
            x2 = min(img_rgb.shape[1]-1, x2); y2 = min(img_rgb.shape[0]-1, y2)
            if x2 > x1 and y2 > y1:
                crop = img_rgb[y1:y2, x1:x2].copy()
                lobes.append(LobeCrop(cls_id=c, box=(x1,y1,x2,y2), crop_rgb=crop))

    # Keep only best per class if duplicates exist (simple rule: largest area)
    best: Dict[int, LobeCrop] = {}
    for lc in lobes:
        x1,y1,x2,y2 = lc.box
        area = (x2-x1)*(y2-y1)
        if (lc.cls_id not in best) or area > (best[lc.cls_id].box[2]-best[lc.cls_id].box[0])*(best[lc.cls_id].box[3]-best[lc.cls_id].box[1]):
            best[lc.cls_id] = lc

    lobes = [best[k] for k in sorted(best.keys())]  # sorted by class id
    return lobes

# --------------------------
# 5) Dataset -> returns 5 crops (or placeholders) + label + original image
# --------------------------
class LobeDataset(Dataset):
    def __init__(self, records, yolo_model):
        self.records = records
        self.yolo = yolo_model

    def __len__(self):
        return len(self.records)

    def __getitem__(self, idx):
        rec = self.records[idx]
        path = rec["ImagePath"]
        label = rec["LabelIdx"]

        img_bgr = cv2.imread(path)
        if img_bgr is None:
            raise FileNotFoundError(path)
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

        lobes = detect_5_lobes(img_rgb, self.yolo, conf_th=CONF_TH)

        # Create fixed 5-slot list by class id [0..4]
        # If missing, slot=None
        slots: List[Optional[LobeCrop]] = [None]*5
        for lc in lobes:
            if 0 <= lc.cls_id < 5:
                slots[lc.cls_id] = lc

        return {
            "path": path,
            "img_rgb": img_rgb,     # for Grad-CAM overlay
            "slots": slots,         # 5 lobes (some may be None)
            "label": label
        }

def collate_fn(batch):
    # keep as list of dicts; model forward will handle variable sizes
    return batch

train_ds = LobeDataset(train_recs, yolo)
val_ds   = LobeDataset(val_recs, yolo)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)
val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)

# --------------------------
# 6) Zone-wise reasoning model ("Chain-of-thought style")
# --------------------------
class ZoneCoTClassifier(nn.Module):
    """
    Step-by-step lobe processing:
      - ResNet18 -> feature per lobe (512)
      - GRU aggregates in lobe order (0..4)
      - classifier uses final hidden state
    Optional: rationale generator using T5, conditioned on per-step hidden state.
    """
    def __init__(self, num_classes: int, hidden_dim: int = 256, use_rationale: bool = True):
        super().__init__()
        self.backbone = ResNet18Backbone(pretrained=True)

        self.feat_proj = nn.Linear(512, hidden_dim)
        self.gru = nn.GRU(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)

        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes)
        )

        self.use_rationale = use_rationale
        if use_rationale:
            self.t5_tokenizer = T5Tokenizer.from_pretrained("t5-small")
            self.t5 = T5ForConditionalGeneration.from_pretrained("t5-small")
            # map hidden state -> T5 d_model (512)
            self.to_t5 = nn.Linear(hidden_dim, self.t5.config.d_model)

    def extract_lobe_feats(self, batch_items):
        """
        Build a tensor [B,5,3,224,224] with zeros where lobe is missing,
        and keep boxes for later Grad-CAM overlay.
        """
        B = len(batch_items)
        imgs = torch.zeros((B, 5, 3, IMG_SIZE, IMG_SIZE), dtype=torch.float32)
        boxes_all: List[List[Optional[Tuple[int,int,int,int]]]] = []

        for i, item in enumerate(batch_items):
            boxes_i = []
            for lobe_id in range(5):
                lc = item["slots"][lobe_id]
                if lc is not None:
                    x = img_tf(lc.crop_rgb)  # tensor [3,224,224]
                    imgs[i, lobe_id] = x
                    boxes_i.append(lc.box)
                else:
                    boxes_i.append(None)
            boxes_all.append(boxes_i)

        return imgs.to(device), boxes_all

    def forward(self, batch_items, rationale_targets: Optional[List[str]] = None):
        """
        batch_items: list of dicts from collate_fn
        rationale_targets: optional list[str] (if you want supervised rationale training)
        """
        imgs_5, boxes_all = self.extract_lobe_feats(batch_items)   # [B,5,3,224,224]
        B = imgs_5.shape[0]

        # Flatten lobes into a single batch for backbone
        flat = imgs_5.view(B*5, 3, IMG_SIZE, IMG_SIZE)
        vec, feat_map = self.backbone(flat)  # vec: [B*5,512]

        vec = vec.view(B, 5, 512)
        z = self.feat_proj(vec)              # [B,5,H]
        z = torch.tanh(z)

        # GRU aggregation in lobe order => "step-by-step reasoning"
        h0 = torch.zeros(1, B, z.size(-1), device=device)
        out, hn = self.gru(z, h0)            # out:[B,5,H], hn:[1,B,H]
        final_h = hn.squeeze(0)              # [B,H]

        logits = self.classifier(final_h)    # [B,C]

        # Optionally generate rationale (one per sample) from the final hidden state
        # You can also generate per-lobe rationale by using out[:,step,:]
        rationale = None
        rationale_loss = None
        if self.use_rationale:
            # encoder embeds length=1
            enc = self.to_t5(final_h).unsqueeze(1)  # [B,1,512]
            if rationale_targets is not None:
                tok = self.t5_tokenizer(
                    rationale_targets, padding=True, truncation=True, max_length=64, return_tensors="pt"
                ).to(device)
                labels = tok.input_ids.clone()
                labels[labels == self.t5_tokenizer.pad_token_id] = -100
                t5_out = self.t5(inputs_embeds=enc, labels=labels)
                rationale_loss = t5_out.loss
            else:
                gen_ids = self.t5.generate(inputs_embeds=enc, max_length=64, num_beams=3)
                rationale = [self.t5_tokenizer.decode(g, skip_special_tokens=True) for g in gen_ids]

        return {
            "logits": logits,
            "boxes": boxes_all,
            "feat_maps": feat_map,  # [B*5,512,h,w] for Grad-CAM
            "flat_imgs": flat,      # [B*5,3,224,224]
            "step_states": out,     # [B,5,H] (zone-wise reasoning states)
            "rationale": rationale,
            "rationale_loss": rationale_loss
        }

# --------------------------
# 7) Grad-CAM for ResNet18 (per lobe)
# --------------------------
class GradCAM:
    def __init__(self, backbone: ResNet18Backbone):
        self.backbone = backbone
        self.gradients = None
        self.activations = None

        # Hook the last conv output = backbone.stem last block output
        # Easiest: register hook on backbone.stem (it returns feat_map already),
        # but we will hook on layer4 output by attaching to last module:
        last_module = list(self.backbone.stem.children())[-1]  # layer4
        last_module.register_forward_hook(self._forward_hook)
        last_module.register_full_backward_hook(self._backward_hook)

    def _forward_hook(self, module, inp, out):
        self.activations = out  # [N,512,H,W]

    def _backward_hook(self, module, grad_in, grad_out):
        self.gradients = grad_out[0]  # [N,512,H,W]

    def __call__(self, x, class_score):
        """
        x: [N,3,224,224]
        class_score: scalar tensor (sum of target class scores)
        Returns CAMs: [N,H,W] normalized
        """
        self.backbone.zero_grad(set_to_none=True)
        class_score.backward(retain_graph=True)

        grads = self.gradients      # [N,512,H,W]
        acts = self.activations     # [N,512,H,W]

        weights = grads.mean(dim=(2,3), keepdim=True)  # [N,512,1,1]
        cam = (weights * acts).sum(dim=1)              # [N,H,W]
        cam = F.relu(cam)

        # normalize each cam
        cam_flat = cam.view(cam.size(0), -1)
        cam_min = cam_flat.min(dim=1)[0].view(-1,1,1)
        cam_max = cam_flat.max(dim=1)[0].view(-1,1,1)
        cam = (cam - cam_min) / (cam_max - cam_min + 1e-8)
        return cam.detach()

def overlay_cam_on_crop(crop_rgb: np.ndarray, cam_2d: np.ndarray, alpha=0.4):
    cam_resized = cv2.resize(cam_2d, (crop_rgb.shape[1], crop_rgb.shape[0]))
    heatmap = cv2.applyColorMap(np.uint8(255*cam_resized), cv2.COLORMAP_JET)
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
    overlay = (1-alpha)*crop_rgb + alpha*heatmap
    overlay = np.clip(overlay, 0, 255).astype(np.uint8)
    return overlay

def paste_crop_back(base_rgb: np.ndarray, crop_overlay: np.ndarray, box):
    x1,y1,x2,y2 = box
    h = y2-y1; w = x2-x1
    crop_overlay = cv2.resize(crop_overlay, (w, h))
    out = base_rgb.copy()
    out[y1:y2, x1:x2] = crop_overlay
    return out

# --------------------------
# 8) Train / Eval
# --------------------------
model = ZoneCoTClassifier(num_classes=num_classes, hidden_dim=256, use_rationale=True).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LR)

def run_epoch(loader, train=True):
    model.train(train)
    total_loss, total_acc, n = 0.0, 0.0, 0

    for batch in loader:
        labels = torch.tensor([b["label"] for b in batch], device=device, dtype=torch.long)

        out = model(batch, rationale_targets=None)  # you can supervise rationale if you have text
        logits = out["logits"]

        loss_cls = criterion(logits, labels)
        loss = loss_cls
        if out["rationale_loss"] is not None:
            # optional: small weight so it doesn't dominate
            loss = loss + 0.3 * out["rationale_loss"]

        if train:
            optimizer.zero_grad(set_to_none=True)
            loss.backward()
            optimizer.step()

        preds = logits.argmax(dim=1)
        total_acc += (preds == labels).sum().item()
        total_loss += loss.item() * labels.size(0)
        n += labels.size(0)

    return total_loss / max(n,1), total_acc / max(n,1)

best_val = 0.0
for epoch in range(1, EPOCHS+1):
    tr_loss, tr_acc = run_epoch(train_loader, train=True)
    va_loss, va_acc = run_epoch(val_loader, train=False)
    print(f"Epoch {epoch:02d} | Train loss {tr_loss:.4f} acc {tr_acc:.4f} | Val loss {va_loss:.4f} acc {va_acc:.4f}")

    if va_acc > best_val:
        best_val = va_acc
        torch.save({"state": model.state_dict(), "label_encoder": le}, "best_zone_cot_resnet18.pt")
        print("✅ saved best: best_zone_cot_resnet18.pt")

print("Best Val Acc:", best_val)

# --------------------------
# 9) Inference: prediction + zone-wise rationale + Grad-CAM overlays
# --------------------------
@torch.no_grad()
def predict_with_zone_cot(img_path: str):
    # build one-item batch in same format as dataset
    img_bgr = cv2.imread(img_path)
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    lobes = detect_5_lobes(img_rgb, yolo, conf_th=CONF_TH)

    slots = [None]*5
    for lc in lobes:
        if 0 <= lc.cls_id < 5:
            slots[lc.cls_id] = lc

    batch = [{"path": img_path, "img_rgb": img_rgb, "slots": slots, "label": 0}]
    out = model(batch, rationale_targets=None)
    logits = out["logits"]
    pred_idx = int(logits.argmax(dim=1).item())
    pred_label = le.inverse_transform([pred_idx])[0]

    # Optional generated rationale (one sentence)
    rationale = out["rationale"][0] if out["rationale"] is not None else None

    # Zone-wise “step states” => you can print short step summaries
    step_states = out["step_states"][0].detach().cpu().numpy()  # [5,H]

    return pred_label, rationale, img_rgb, slots, step_states

def gradcam_visualize_lobes(img_rgb, slots, target_class_idx: int, save_path: Optional[str]=None):
    """
    Produces:
      - per-lobe Grad-CAM overlay on each crop
      - pasted back onto original image
    """
    # Build lobe tensor [5,3,224,224]
    crops_t = []
    boxes = []
    for lobe_id in range(5):
        lc = slots[lobe_id]
        if lc is not None:
            crops_t.append(img_tf(lc.crop_rgb))
            boxes.append(lc.box)
        else:
            crops_t.append(torch.zeros(3, IMG_SIZE, IMG_SIZE))
            boxes.append(None)

    x = torch.stack(crops_t, dim=0).to(device)  # [5,3,224,224]
    x.requires_grad_(True)

    # Forward through backbone + compute a "target score" using the full model
    # We need full model forward, but it expects batch_items; easiest:
    # Create a fake batch and run model, then backprop from target class score.
    fake_batch = [{"path": "x", "img_rgb": img_rgb, "slots": slots, "label": 0}]
    out = model(fake_batch, rationale_targets=None)
    score = out["logits"][:, target_class_idx].sum()

    # Grad-CAM on backbone (per lobe crop) using hooks
    cam_engine = GradCAM(model.backbone)
    # Re-run backbone forward on x so hooks see it (same backbone object)
    vec, _ = model.backbone(x)
    # Reuse the same target score from full model (already computed) for gradients
    cams = cam_engine(x, score)  # [5,h,w]

    # Overlay per lobe and paste back
    composed = img_rgb.copy()
    for lobe_id in range(5):
        lc = slots[lobe_id]
        if lc is None:
            continue
        cam2d = cams[lobe_id].detach().cpu().numpy()
        overlay_crop = overlay_cam_on_crop(lc.crop_rgb, cam2d, alpha=0.45)
        composed = paste_crop_back(composed, overlay_crop, lc.box)

    if save_path is not None:
        bgr = cv2.cvtColor(composed, cv2.COLOR_RGB2BGR)
        cv2.imwrite(save_path, bgr)

    return composed

"""# --------------------------
# 10) Example usage
# --------------------------
# test_img = "/content/drive/MyDrive/Dataset lable/Dataset lable/Pneumoina-Viral/Pneumonia-Viral (10).jpg"
# pred_label, rationale, img_rgb, slots, step_states = predict_with_zone_cot(test_img)
# print("Pred:", pred_label)
# print("Rationale:", rationale)
#
# pred_idx = int(le.transform([pred_label])[0])
# overlay_img = gradcam_visualize_lobes(img_rgb, slots, target_class_idx=pred_idx, save_path="gradcam_lobes_overlay.jpg")
# print("Saved: gradcam_lobes_overlay.jpg")
"""

